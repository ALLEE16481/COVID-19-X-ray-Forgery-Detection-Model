{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkPq/9hWc1TzFsGPNPiZJZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALLEE16481/COVID-19-X-ray-Forgery-Detection-Model/blob/main/COVID_19_X_ray_Forgery_Detection_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic scientific stack (already installed usually, but just in case)\n",
        "!pip install numpy pandas matplotlib seaborn scikit-learn\n",
        "\n",
        "# TensorFlow (pre-installed in Colab, but if you want a specific version)\n",
        "!pip install tensorflow\n",
        "\n",
        "# OpenCV\n",
        "!pip install opencv-python-headless\n",
        "\n",
        "# Requests (usually installed, but just in case)\n",
        "!pip install requests\n",
        "\n",
        "# pathlib is part of Python standard library ‚Äî no install needed.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap1BDfxd17Vc",
        "outputId": "6bf7a6b8-f6dd-4e6f-8929-8eb0b79610b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "TG0OE7eC2_9_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"üì¶ All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7y9Ust1lqM9",
        "outputId": "80b62a78-c4fa-48ed-f6a9-4141fda1a33a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"üì¶ All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPe1iVkzl2re",
        "outputId": "1b55e1ca-d869-49de-9834-92ad8bef39d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def create_synthetic_dataset(self, samples_per_class=200):\n",
        "        \"\"\"\n",
        "        Create synthetic dataset for demonstration (replace with real data loading)\n",
        "        \"\"\"\n",
        "        print(\"üé≤ Creating synthetic dataset for demonstration...\")\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        class_names = ['COVID-19', 'Viral_Pneumonia', 'Normal']\n",
        "\n",
        "        for class_name in class_names:\n",
        "            print(f\"   Generating {samples_per_class} samples for {class_name}\")\n",
        "\n",
        "            for i in range(samples_per_class):\n",
        "                # Generate realistic-looking X-ray synthetic data\n",
        "                base_img = np.random.normal(0.3, 0.1, (*self.img_size, 3))\n",
        "\n",
        "                # Add class-specific patterns\n",
        "                if class_name == 'COVID-19':\n",
        "                    # Add some ground-glass opacity patterns\n",
        "                    for _ in range(5):\n",
        "                        x, y = np.random.randint(50, self.img_size[0]-50, 2)\n",
        "                        cv2.circle(base_img, (x, y), np.random.randint(10, 30),\n",
        "                                 (0.6, 0.6, 0.6), -1)\n",
        "\n",
        "                elif class_name == 'Viral_Pneumonia':\n",
        "                    # Add consolidation patterns\n",
        "                    for _ in range(3):\n",
        "                        x, y = np.random.randint(30, self.img_size[0]-30, 2)\n",
        "                        cv2.rectangle(base_img, (x, y),\n",
        "                                    (x+40, y+40), (0.7, 0.7, 0.7), -1)\n",
        "\n",
        "                # Normalize and clip\n",
        "                base_img = np.clip(base_img, 0, 1)\n",
        "                images.append(base_img)\n",
        "                labels.append(class_name)\n",
        "\n",
        "        self.X = np.array(images, dtype=np.float32)\n",
        "        self.y_raw = np.array(labels)\n",
        "\n",
        "        # Encode labels\n",
        "        self.y_encoded = self.label_encoder.fit_transform(self.y_raw)\n",
        "        self.y_categorical = to_categorical(self.y_encoded, num_classes=3)\n",
        "\n",
        "        print(f\"‚úÖ Dataset created: {len(self.X)} images\")\n",
        "        print(f\"   Classes: {list(self.label_encoder.classes_)}\")\n",
        "\n",
        "        # Return the data as well, although it's now stored in self.X and self.y_categorical\n",
        "        return self.X, self.y_categorical"
      ],
      "metadata": {
        "id": "8FHcjPbRmN1x"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def download_kaggle_dataset(self):\n",
        "        \"\"\"\n",
        "        Download the actual Kaggle dataset (requires Kaggle API setup)\n",
        "        \"\"\"\n",
        "        if not IN_COLAB:\n",
        "            print(\"‚ö†Ô∏è  This function is designed for Google Colab\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Mount Google Drive to access Kaggle credentials\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "            # Install Kaggle\n",
        "            os.system('pip install kaggle')\n",
        "\n",
        "            # Setup Kaggle credentials\n",
        "            os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "\n",
        "            # Download dataset\n",
        "            os.system('kaggle datasets download -d nourmahmoud/covid19-digital-xrays-forgery-dataset')\n",
        "\n",
        "            # Extract dataset\n",
        "            with zipfile.ZipFile('covid19-digital-xrays-forgery-dataset.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content/dataset')\n",
        "\n",
        "            print(\"‚úÖ Kaggle dataset downloaded and extracted!\")\n",
        "            return '/content/dataset'\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error downloading dataset: {e}\")\n",
        "            print(\"üí° Using synthetic data instead...\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "UnqmBefnmZrw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def visualize_samples(self, num_samples=8):\n",
        "        \"\"\"\n",
        "        Visualize sample images\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        random_indices = random.sample(range(len(self.X)), min(num_samples, len(self.X)))\n",
        "\n",
        "        for i, idx in enumerate(random_indices):\n",
        "            plt.subplot(2, 4, i + 1)\n",
        "            plt.imshow(self.X[idx])\n",
        "            plt.title(f'{self.y_raw[idx]}')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle('Sample X-ray Images', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7SekBKgvmdNM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def split_data(self, test_size=0.2, val_size=0.1):\n",
        "        \"\"\"\n",
        "        Split data with stratification\n",
        "        \"\"\"\n",
        "        print(\"üìä Splitting data...\")\n",
        "\n",
        "        # First split\n",
        "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
        "            self.X, self.y_categorical, test_size=test_size,\n",
        "            random_state=42, stratify=self.y_categorical\n",
        "        )\n",
        "\n",
        "        # Second split\n",
        "        val_size_adj = val_size / (1 - test_size)\n",
        "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=val_size_adj,\n",
        "            random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        print(f\"   Training: {self.X_train.shape[0]} samples\")\n",
        "        print(f\"   Validation: {self.X_val.shape[0]} samples\")\n",
        "        print(f\"   Test: {self.X_test.shape[0]} samples\")"
      ],
      "metadata": {
        "id": "xZqORAXOmlXp"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def build_optimized_model(self):\n",
        "        \"\"\"\n",
        "        Build Colab-optimized CNN model\n",
        "        \"\"\"\n",
        "        print(\"üèóÔ∏è  Building optimized CNN model...\")\n",
        "\n",
        "        model = Sequential([\n",
        "            # Efficient convolutional base\n",
        "            Conv2D(32, (3, 3), activation='relu', input_shape=(*self.img_size, 3)),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(64, (3, 3), activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(128, (3, 3), activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            # Global average pooling instead of flatten (more efficient)\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "            # Dense layers as specified in your requirements\n",
        "            Dense(300, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "\n",
        "            Dense(150, activation='relu'),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            Dense(75, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(50, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(25, activation='relu'),\n",
        "            Dropout(0.1),\n",
        "\n",
        "            # Output layer with float32 for stability\n",
        "            Dense(3, activation='softmax', dtype='float32')\n",
        "        ])\n",
        "\n",
        "        # Use Adam with learning rate scheduling\n",
        "        optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        print(\"‚úÖ Model built successfully!\")\n",
        "\n",
        "        # Display model summary\n",
        "        model.summary()\n",
        "        return model"
      ],
      "metadata": {
        "id": "HTaU-h48msnZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def create_data_generators(self):\n",
        "        \"\"\"\n",
        "        Create memory-efficient data generators\n",
        "        \"\"\"\n",
        "        # Conservative augmentation for Colab\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            zoom_range=0.1,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        val_datagen = ImageDataGenerator()\n",
        "\n",
        "        self.train_gen = train_datagen.flow(\n",
        "            self.X_train, self.y_train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        self.val_gen = val_datagen.flow(\n",
        "            self.X_val, self.y_val,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        return self.train_gen, self.val_gen"
      ],
      "metadata": {
        "id": "pylIcjuLmygg"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def train_with_callbacks(self, epochs=30):\n",
        "        \"\"\"\n",
        "        Train with Colab-optimized callbacks\n",
        "        \"\"\"\n",
        "        print(f\"üöÇ Training model for {epochs} epochs...\")\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=8,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.3,\n",
        "                patience=4,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                'best_model.h5',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Calculate steps\n",
        "        steps_per_epoch = len(self.X_train) // self.batch_size\n",
        "        validation_steps = len(self.X_val) // self.batch_size\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            self.train_gen,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=epochs,\n",
        "            validation_data=self.val_gen,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "fVYxaB8tm349"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "\n",
        "    def evaluate_and_visualize(self):\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation with visualizations\n",
        "        \"\"\"\n",
        "        print(\"üìà Evaluating model performance...\")\n",
        "\n",
        "        # Predictions\n",
        "        test_pred = self.model.predict(self.X_test, batch_size=self.batch_size)\n",
        "        test_pred_classes = np.argmax(test_pred, axis=1)\n",
        "        test_true_classes = np.argmax(self.y_test, axis=1)\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = accuracy_score(test_true_classes, test_pred_classes)\n",
        "        print(f\"üéØ Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Create comprehensive visualization\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # 1. Training History\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Training')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')\n",
        "        axes[0, 0].set_title('Model Accuracy')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # 2. Loss History\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Training')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Validation')\n",
        "        axes[0, 1].set_title('Model Loss')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True)\n",
        "\n",
        "        # 3. Confusion Matrix\n",
        "        cm = confusion_matrix(test_true_classes, test_pred_classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
        "                   xticklabels=self.label_encoder.classes_,\n",
        "                   yticklabels=self.label_encoder.classes_)\n",
        "        axes[1, 0].set_title('Confusion Matrix')\n",
        "        axes[1, 0].set_ylabel('True Label')\n",
        "        axes[1, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "        # 4. Class Distribution\n",
        "        unique, counts = np.unique(test_pred_classes, return_counts=True)\n",
        "        axes[1, 1].bar([self.label_encoder.classes_[i] for i in unique], counts)\n",
        "        axes[1, 1].set_title('Prediction Distribution')\n",
        "        axes[1, 1].set_ylabel('Count')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Classification Report\n",
        "        print(\"\\nüìä Detailed Classification Report:\")\n",
        "        print(classification_report(test_true_classes, test_pred_classes,\n",
        "                                  target_names=self.label_encoder.classes_))\n",
        "\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "dl4gimmMnesc"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def predict_sample(self, image_idx=None):\n",
        "        \"\"\"\n",
        "        Predict and visualize a sample\n",
        "        \"\"\"\n",
        "        if image_idx is None:\n",
        "            image_idx = random.randint(0, len(self.X_test) - 1)\n",
        "\n",
        "        img = self.X_test[image_idx:image_idx+1]\n",
        "        true_label = self.label_encoder.classes_[np.argmax(self.y_test[image_idx])]\n",
        "\n",
        "        pred = self.model.predict(img, verbose=0)\n",
        "        pred_label = self.label_encoder.classes_[np.argmax(pred[0])]\n",
        "        confidence = np.max(pred[0])\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Show image\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(self.X_test[image_idx])\n",
        "        plt.title(f'True: {true_label}\\nPredicted: {pred_label}\\nConfidence: {confidence:.2f}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Show prediction probabilities\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.bar(self.label_encoder.classes_, pred[0])\n",
        "        plt.title('Prediction Probabilities')\n",
        "        plt.ylabel('Probability')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return pred_label, confidence"
      ],
      "metadata": {
        "id": "apZXw1w9nm9D"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd8d519e"
      },
      "source": [
        "class ColabForgeryDetector:\n",
        "    def __init__(self, img_size=(224, 224), batch_size=16):\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.X = None\n",
        "        self.y_raw = None\n",
        "        self.y_encoded = None\n",
        "        self.y_categorical = None\n",
        "        self.X_train = None\n",
        "        self.X_val = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_val = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.train_gen = None\n",
        "        self.val_gen = None\n",
        "\n",
        "    def create_synthetic_dataset(self, samples_per_class=200):\n",
        "        \"\"\"\n",
        "        Create synthetic dataset for demonstration (replace with real data loading)\n",
        "        \"\"\"\n",
        "        print(\"üé≤ Creating synthetic dataset for demonstration...\")\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        class_names = ['COVID-19', 'Viral_Pneumonia', 'Normal']\n",
        "\n",
        "        for class_name in class_names:\n",
        "            print(f\"   Generating {samples_per_class} samples for {class_name}\")\n",
        "\n",
        "            for i in range(samples_per_class):\n",
        "                # Generate realistic-looking X-ray synthetic data\n",
        "                base_img = np.random.normal(0.3, 0.1, (*self.img_size, 3))\n",
        "\n",
        "                # Add class-specific patterns\n",
        "                if class_name == 'COVID-19':\n",
        "                    # Add some ground-glass opacity patterns\n",
        "                    for _ in range(5):\n",
        "                        x, y = np.random.randint(50, self.img_size[0]-50, 2)\n",
        "                        cv2.circle(base_img, (x, y), np.random.randint(10, 30),\n",
        "                                 (0.6, 0.6, 0.6), -1)\n",
        "\n",
        "                elif class_name == 'Viral_Pneumonia':\n",
        "                    # Add consolidation patterns\n",
        "                    for _ in range(3):\n",
        "                        x, y = np.random.randint(30, self.img_size[0]-30, 2)\n",
        "                        cv2.rectangle(base_img, (x, y),\n",
        "                                    (x+40, y+40), (0.7, 0.7, 0.7), -1)\n",
        "\n",
        "                # Normalize and clip\n",
        "                base_img = np.clip(base_img, 0, 1)\n",
        "                images.append(base_img)\n",
        "                labels.append(class_name)\n",
        "\n",
        "        self.X = np.array(images, dtype=np.float32)\n",
        "        self.y_raw = np.array(labels)\n",
        "\n",
        "        # Encode labels\n",
        "        self.y_encoded = self.label_encoder.fit_transform(self.y_raw)\n",
        "        self.y_categorical = to_categorical(self.y_encoded, num_classes=3)\n",
        "\n",
        "        print(f\"‚úÖ Dataset created: {len(self.X)} images\")\n",
        "        print(f\"   Classes: {list(self.label_encoder.classes_)}\")\n",
        "\n",
        "        # Return the data as well, although it's now stored in self.X and self.y_categorical\n",
        "        return self.X, self.y_categorical\n",
        "\n",
        "    def download_kaggle_dataset(self):\n",
        "        \"\"\"\n",
        "        Download the actual Kaggle dataset (requires Kaggle API setup)\n",
        "        \"\"\"\n",
        "        if not IN_COLAB:\n",
        "            print(\"‚ö†Ô∏è  This function is designed for Google Colab\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Mount Google Drive to access Kaggle credentials\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "            # Install Kaggle\n",
        "            os.system('pip install kaggle')\n",
        "\n",
        "            # Setup Kaggle credentials\n",
        "            os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "\n",
        "            # Download dataset\n",
        "            os.system('kaggle datasets download -d nourmahmoud/covid19-digital-xrays-forgery-dataset')\n",
        "\n",
        "            # Extract dataset\n",
        "            with zipfile.ZipFile('covid19-digital-xrays-forgery-dataset.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content/dataset')\n",
        "\n",
        "            print(\"‚úÖ Kaggle dataset downloaded and extracted!\")\n",
        "            return '/content/dataset'\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error downloading dataset: {e}\")\n",
        "            print(\"üí° Using synthetic data instead...\")\n",
        "            return None\n",
        "\n",
        "    def visualize_samples(self, num_samples=8):\n",
        "        \"\"\"\n",
        "        Visualize sample images\n",
        "        \"\"\"\n",
        "        if self.X is None or self.y_raw is None:\n",
        "             print(\"‚ö†Ô∏è Dataset not loaded. Please create or download the dataset first.\")\n",
        "             return\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        random_indices = random.sample(range(len(self.X)), min(num_samples, len(self.X)))\n",
        "\n",
        "        for i, idx in enumerate(random_indices):\n",
        "            plt.subplot(2, 4, i + 1)\n",
        "            plt.imshow(self.X[idx])\n",
        "            plt.title(f'{self.y_raw[idx]}')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle('Sample X-ray Images', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def split_data(self, test_size=0.2, val_size=0.1):\n",
        "        \"\"\"\n",
        "        Split data with stratification\n",
        "        \"\"\"\n",
        "        if self.X is None or self.y_categorical is None:\n",
        "            print(\"‚ö†Ô∏è Dataset not loaded. Please create or download the dataset first.\")\n",
        "            return\n",
        "\n",
        "        print(\"üìä Splitting data...\")\n",
        "\n",
        "        # First split\n",
        "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
        "            self.X, self.y_categorical, test_size=test_size,\n",
        "            random_state=42, stratify=self.y_categorical\n",
        "        )\n",
        "\n",
        "        # Second split\n",
        "        val_size_adj = val_size / (1 - test_size)\n",
        "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=val_size_adj,\n",
        "            random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        print(f\"   Training: {self.X_train.shape[0]} samples\")\n",
        "        print(f\"   Validation: {self.X_val.shape[0]} samples\")\n",
        "        print(f\"   Test: {self.X_test.shape[0]} samples\")\n",
        "\n",
        "    def build_optimized_model(self):\n",
        "        \"\"\"\n",
        "        Build Colab-optimized CNN model\n",
        "        \"\"\"\n",
        "        print(\"üèóÔ∏è  Building optimized CNN model...\")\n",
        "\n",
        "        model = Sequential([\n",
        "            # Efficient convolutional base\n",
        "            Conv2D(32, (3, 3), activation='relu', input_shape=(*self.img_size, 3)),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(64, (3, 3), activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(128, (3, 3), activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            # Global average pooling instead of flatten (more efficient)\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "            # Dense layers as specified in your requirements\n",
        "            Dense(300, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "\n",
        "            Dense(150, activation='relu'),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            Dense(75, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(50, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(25, activation='relu'),\n",
        "            Dropout(0.1),\n",
        "\n",
        "            # Output layer with float32 for stability\n",
        "            Dense(3, activation='softmax', dtype='float32')\n",
        "        ])\n",
        "\n",
        "        # Use Adam with learning rate scheduling\n",
        "        optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        print(\"‚úÖ Model built successfully!\")\n",
        "\n",
        "        # Display model summary\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def create_data_generators(self):\n",
        "        \"\"\"\n",
        "        Create memory-efficient data generators\n",
        "        \"\"\"\n",
        "        if self.X_train is None or self.y_train is None or self.X_val is None or self.y_val is None:\n",
        "             print(\"‚ö†Ô∏è Data not split. Please split the data first.\")\n",
        "             return None, None\n",
        "\n",
        "        # Conservative augmentation for Colab\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            zoom_range=0.1,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        val_datagen = ImageDataGenerator()\n",
        "\n",
        "        self.train_gen = train_datagen.flow(\n",
        "            self.X_train, self.y_train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        self.val_gen = val_datagen.flow(\n",
        "            self.X_val, self.y_val,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        return self.train_gen, self.val_gen\n",
        "\n",
        "    def train_with_callbacks(self, epochs=30):\n",
        "        \"\"\"\n",
        "        Train with Colab-optimized callbacks\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ö†Ô∏è Model not built. Please build the model first.\")\n",
        "            return None\n",
        "        if self.train_gen is None or self.val_gen is None:\n",
        "             print(\"‚ö†Ô∏è Data generators not created. Please create data generators first.\")\n",
        "             return None\n",
        "\n",
        "\n",
        "        print(f\"üöÇ Training model for {epochs} epochs...\")\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=8,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.3,\n",
        "                patience=4,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                'best_model.h5',\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Calculate steps\n",
        "        steps_per_epoch = len(self.X_train) // self.batch_size\n",
        "        validation_steps = len(self.X_val) // self.batch_size\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            self.train_gen,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=epochs,\n",
        "            validation_data=self.val_gen,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Training completed!\")\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_and_visualize(self):\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation with visualizations\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ö†Ô∏è Model not built. Please build the model first.\")\n",
        "            return None\n",
        "        if self.X_test is None or self.y_test is None:\n",
        "             print(\"‚ö†Ô∏è Test data not available. Please split the data and train the model first.\")\n",
        "             return None\n",
        "        if self.history is None:\n",
        "            print(\"‚ö†Ô∏è Model not trained. Please train the model first.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        print(\"üìà Evaluating model performance...\")\n",
        "\n",
        "        # Predictions\n",
        "        test_pred = self.model.predict(self.X_test, batch_size=self.batch_size)\n",
        "        test_pred_classes = np.argmax(test_pred, axis=1)\n",
        "        test_true_classes = np.argmax(self.y_test, axis=1)\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = accuracy_score(test_true_classes, test_pred_classes)\n",
        "        print(f\"üéØ Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Create comprehensive visualization\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # 1. Training History\n",
        "        axes[0, 0].plot(self.history.history['accuracy'], label='Training')\n",
        "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')\n",
        "        axes[0, 0].set_title('Model Accuracy')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # 2. Loss History\n",
        "        axes[0, 1].plot(self.history.history['loss'], label='Training')\n",
        "        axes[0, 1].plot(self.history.history['val_loss'], label='Validation')\n",
        "        axes[0, 1].set_title('Model Loss')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True)\n",
        "\n",
        "        # 3. Confusion Matrix\n",
        "        cm = confusion_matrix(test_true_classes, test_pred_classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
        "                   xticklabels=self.label_encoder.classes_,\n",
        "                   yticklabels=self.label_encoder.classes_)\n",
        "        axes[1, 0].set_title('Confusion Matrix')\n",
        "        axes[1, 0].set_ylabel('True Label')\n",
        "        axes[1, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "        # 4. Class Distribution\n",
        "        unique, counts = np.unique(test_pred_classes, return_counts=True)\n",
        "        axes[1, 1].bar([self.label_encoder.classes_[i] for i in unique], counts)\n",
        "        axes[1, 1].set_title('Prediction Distribution')\n",
        "        axes[1, 1].set_ylabel('Count')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Classification Report\n",
        "        print(\"\\nüìä Detailed Classification Report:\")\n",
        "        print(classification_report(test_true_classes, test_pred_classes,\n",
        "                                  target_names=self.label_encoder.classes_))\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def predict_sample(self, image_idx=None):\n",
        "        \"\"\"\n",
        "        Predict and visualize a sample\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ö†Ô∏è Model not built. Please build and train the model first.\")\n",
        "            return None, None\n",
        "        if self.X_test is None or self.y_test is None:\n",
        "             print(\"‚ö†Ô∏è Test data not available. Please split the data and train the model first.\")\n",
        "             return None, None\n",
        "\n",
        "        if image_idx is None:\n",
        "            image_idx = random.randint(0, len(self.X_test) - 1)\n",
        "\n",
        "        img = self.X_test[image_idx:image_idx+1]\n",
        "        true_label = self.label_encoder.classes_[np.argmax(self.y_test[image_idx])]\n",
        "\n",
        "        pred = self.model.predict(img, verbose=0)\n",
        "        pred_label = self.label_encoder.classes_[np.argmax(pred[0])]\n",
        "        confidence = np.max(pred[0])\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Show image\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(self.X_test[image_idx])\n",
        "        plt.title(f'True: {true_label}\\nPredicted: {pred_label}\\nConfidence: {confidence:.2f}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Show prediction probabilities\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.bar(self.label_encoder.classes_, pred[0])\n",
        "        plt.title('Prediction Probabilities')\n",
        "        plt.ylabel('Probability')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return pred_label, confidence"
      ],
      "execution_count": 65,
      "outputs": []
    }
  ]
}